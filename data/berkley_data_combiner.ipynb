{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "import importlib\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Explicitely For Crawling\n",
    "import requests\n",
    "import sys\n",
    "from lxml import etree\n",
    "import asyncio\n",
    "import aiohttp  # pip install aiohttp\n",
    "import aiofiles  # pip install aiofiles\n",
    "\n",
    "import compress as comp\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "OUTPUT_FILE_PATH = \"sources\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Station Summaries\n",
    "Since there is no already zipped file of the station data @http://berkeleyearth.lbl.gov/auto/Stations/TAVG/Text/, I had to write a little crawler which downloads all of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If started you need to completely kill the kernel to get rid of the asynchronous download tasks\n",
    "LOCAL_FILE_PATH = \"sources/berkley_station_summaries/\"\n",
    "WEB_FILE_PATH = \"http://berkeleyearth.lbl.gov/auto/Stations/TAVG/Text/\"\n",
    "\n",
    "def getSiteDom(url):\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code != 200:\n",
    "        print(f\"Couldn't download url {url}\")\n",
    "        return None\n",
    "    return etree.HTML(resp.text)\n",
    "\n",
    "def downloadFile(url, lpath):\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code != 200:\n",
    "        print(f\"Couldn't download file {url}\")\n",
    "        return\n",
    "    open(lpath, \"wb\").write(resp.content)\n",
    "\n",
    "def downloadFilesParallel(urls, lpaths, maxp = 10):\n",
    "    sema = asyncio.BoundedSemaphore(maxp)\n",
    "    pbar = tqdm(total=len(urls), desc=\"Downloading files\")\n",
    "    async def fetch_file(url, lpath):\n",
    "        async with sema, aiohttp.ClientSession() as session:\n",
    "            async with session.get(url) as resp:\n",
    "                if resp.status != 200:\n",
    "                    print(f\" -> Couldn't download file {url}\")\n",
    "                    pbar.update(1)\n",
    "                    return\n",
    "                data = await resp.read()\n",
    "        async with aiofiles.open(lpath, \"wb\") as outfile:\n",
    "            await outfile.write(data)\n",
    "            pbar.update(1)\n",
    "\n",
    "    async def fetch_files(urls, lpaths):\n",
    "        tasks = [asyncio.ensure_future(fetch_file(url,lpath)) for url,lpath in zip(urls, lpaths)]\n",
    "        await asyncio.gather(*tasks)\n",
    "    \n",
    "    loop = asyncio.get_event_loop()\n",
    "    try:\n",
    "        loop.run_until_complete(fetch_files(urls, lpaths))\n",
    "    finally:\n",
    "        loop.run_until_complete(loop.shutdown_asyncgens())\n",
    "        loop.close()\n",
    "    pbar.close()\n",
    "\n",
    "def getAllFileNames(url):\n",
    "    dom = getSiteDom(url)\n",
    "    elements = dom.xpath(\"//td/a[contains(@href,'.txt')]\")\n",
    "    ret = []\n",
    "    for element in elements:\n",
    "        ret.append(element.get(\"href\"))\n",
    "    return ret\n",
    "\n",
    "filenames = getAllFileNames(WEB_FILE_PATH)\n",
    "#filenames = filenames[slice(20)]\n",
    "urls = [WEB_FILE_PATH + filename for filename in filenames]\n",
    "lpaths = [LOCAL_FILE_PATH + filename for filename in filenames]\n",
    "files = len(os.listdir(LOCAL_FILE_PATH))\n",
    "if (len(os.listdir(LOCAL_FILE_PATH)) >= len(urls)):\n",
    "    print(\"Aborted because more or the same amount of files exists in the download folder than there are to download.\")\n",
    "#downloadFilesParallel(urls, lpaths, maxp=20)\n",
    "#for filename in tqdm(filenames, desc=\"Downloading files\"):\n",
    "#    url = WEB_FILE_PATH + filename\n",
    "#    localFile = LOCAL_FILE_PATH + filename\n",
    "#    downloadFile(url, localFile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Station Summaries\n",
    "Imports all files inside `data\\sources\\berkley_station_summaries`. Buffers it in a binary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported from binary file!!!\n"
     ]
    }
   ],
   "source": [
    "LOCAL_FILE_PATH = \"sources/berkley_station_summaries\"\n",
    "OUTPUT_FILE_BINARY = \"sources/stationummaries.parquet.gzip\"\n",
    "\n",
    "NEW_PART_EVERY = 1000\n",
    "FORCE_RECREATION = False\n",
    "\n",
    "#name = 0.80N-8.84E\n",
    "def getDataForOnePosition(id):\n",
    "    file = f\"{LOCAL_FILE_PATH}/{id}-TAVG-Data.txt\"\n",
    "\n",
    "    # Extract Data from comment at begin of trend file:\n",
    "    chunk = \"\"\n",
    "    with open(file) as myFile:\n",
    "        chunk = myFile.read(4069)\n",
    "\n",
    "    #x = re.findall(r\"for the location:[\\s%]+([\\d.\\dNSEW ]+),([\\d.\\dNSEW ]+)\", chunk); #for location\n",
    "    lat = re.findall(r\"Latitude:\\s+([\\w\\S ]+) \\+\", chunk)\n",
    "    lat = lat[0] if len(lat) > 0 else \"\"\n",
    "    lon = re.findall(r\"Longitude:\\s+([\\w\\S ]+) \\+\", chunk)\n",
    "    lon = lon[0] if len(lon) > 0 else \"\"\n",
    "    if lat == \"\" or lon == \"\":\n",
    "        #print(f\"Couldn't  evaluate latitude and longitude for station {id}\")\n",
    "        return None\n",
    "\n",
    "    header = [\"Year\", \"Month\", \"Raw Data Temperature\", \"Raw Data Anomaly\", \"QC Failed\",   \"Continuity Breaks\", \"Adjusted Data Temperature\", \"Adjusted Data Anomaly\", \"Regional Temperature\", \"Regional Anomaly\"]\n",
    "    df = pd.read_csv(file, delimiter=\"\\s+\", header=None, comment=\"%\", names=header, usecols = [0,1,2,3,6,7,8,9], encoding='latin-1')\n",
    "\n",
    "    df[\"lat\"] = lat\n",
    "    df[\"lon\"] = lon\n",
    "    df[\"stationId\"] = id\n",
    "    return df\n",
    "\n",
    "\n",
    "def getIDsFromDirectory(dir):\n",
    "    ids = set()\n",
    "    for path in os.listdir(dir):\n",
    "        if not os.path.isfile(os.path.join(dir, path)):\n",
    "            continue\n",
    "        if not \"-TAVG-Data.txt\" in path:\n",
    "            continue\n",
    "        ids.add(path.replace(\"-TAVG-Data.txt\", \"\"))\n",
    "    return ids\n",
    "\n",
    "if os.path.isfile(OUTPUT_FILE_BINARY) and not FORCE_RECREATION:\n",
    "    df_merged = pd.read_parquet(OUTPUT_FILE_BINARY)\n",
    "    print(\"Data imported from binary file!!!\")\n",
    "else:\n",
    "    allIds = getIDsFromDirectory(LOCAL_FILE_PATH)\n",
    "    idCount = len(allIds)\n",
    "    partCount = math.ceil(idCount / NEW_PART_EVERY)\n",
    "    curPartId = 0\n",
    "    df_parts = [ pd.DataFrame({'A':[]}) for _ in range(partCount) ]\n",
    "    sumEntries = 0\n",
    "    pbar = tqdm(total=idCount, desc=\"Merging files\")\n",
    "    for ind, id in enumerate(allIds):\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f\"Merging files (Counts - Part:{len(df_parts[curPartId])}, Overall:{len(df_parts[curPartId]) + sumEntries}) [Part {curPartId + 1}/{partCount}]\")\n",
    "        #pbar.write(f\"Working on {positionName}...\")\n",
    "        df = getDataForOnePosition(id)\n",
    "        if not df is None and len(df) > 0:\n",
    "            #pbar.write(f\" -> Read {len(df)} entries\")\n",
    "            if len(df_parts[curPartId]) > 0:\n",
    "                df_parts[curPartId] = pd.concat([df_parts[curPartId], df], ignore_index=True)\n",
    "            else:\n",
    "                df_parts[curPartId] = df\n",
    "        \n",
    "        if ind > 0 and ind % NEW_PART_EVERY == 0:\n",
    "            sumEntries += len(df_parts[curPartId])\n",
    "            curPartId += 1\n",
    "    pbar.close()\n",
    "\n",
    "    df_merged = pd.DataFrame({'A':[]})\n",
    "    for pId in tqdm(range(partCount), desc=\"Merging dataframe parts\"):\n",
    "        if len(df_merged) > 0:\n",
    "            df_merged = pd.concat([df_merged, df_parts[pId]], ignore_index=True)\n",
    "        else:\n",
    "            df_merged = df_parts[pId]\n",
    "\n",
    "    df_parts = None\n",
    "    print(\"Saving to binary file...\")\n",
    "    df_merged.to_parquet(OUTPUT_FILE_BINARY, compression=\"gzip\")\n",
    "\n",
    "df_stations = df_merged"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Output File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_stations\n",
    "df_data.rename(columns={\"Adjusted Data Temperature\":\"AverageTemperature\", \"lat\":\"Latitude\", \"lon\":\"Longitude\"}, inplace=True)\n",
    "df_data = df_data[[\"Year\", \"Month\", \"AverageTemperature\", \"Latitude\", \"Longitude\"]]\n",
    "comp.compress_dataset(df_data, output_path=OUTPUT_FILE_PATH, discretizeresolution=2, compressionpreset=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Import Local Summaries\n",
    "Imports all %position%-TAVG-Counts and %position%-TAVG-Trend Text-Files in a certain path and combines all the data in one dataframe. This dataframe is then being saved as a binary file. (Since the process takes quite some time and the dataset is huge)\n",
    "The data can be downloaded here: http://berkeleyearth.lbl.gov/auto/Local/TAVG/Text/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_FILE_PATH = \"sources/berkley_local_summaries\"\n",
    "OUTPUT_FILE_BINARY = \"sources/localsummaries.parquet.gzip\"\n",
    "FORCE_RECREATION = False\n",
    "\n",
    "#name = 0.80N-8.84E\n",
    "def getDataForOnePosition(name):\n",
    "    count_file = f\"{LOCAL_FILE_PATH}/{name}-TAVG-Counts.txt\"\n",
    "    trends_file = f\"{LOCAL_FILE_PATH}/{name}-TAVG-Trend.txt\"\n",
    "\n",
    "    # Extract Data from comment at begin of trend file:\n",
    "    chunk = \"\"\n",
    "    with open(trends_file) as myFile:\n",
    "        chunk = myFile.read(4069)\n",
    "\n",
    "    #x = re.findall(r\"for the location:[\\s%]+([\\d.\\dNSEW ]+),([\\d.\\dNSEW ]+)\", chunk); #for location\n",
    "    country = re.findall(r\"Country: ([\\w\\S ]+)\", chunk) #for Country\n",
    "    country = country[0] if len(country) > 0 else \"\"\n",
    "    citys = re.findall(r\"Nearby Cities: ([\\w\\S ]+)\", chunk)\n",
    "    citys = citys[0] if len(citys) > 0 else \"\"\n",
    "    temperatureList = re.findall(r\"Jan[\\s]+Feb[\\s]+Mar[\\s]+Apr[\\s]+May[\\s]+Jun[\\s]+Jul[\\s]+Aug[\\s]+Sep[\\s]+Oct[\\s]+Nov[\\s]+Dec\\s+%%([\\w\\s\\S]+)%%\", chunk)[0].strip().split(\" \")\n",
    "    temperatureList = list(filter(None, temperatureList))\n",
    "    temperatureList = [eval(i) for i in temperatureList]\n",
    "\n",
    "    latlon = name.split(\"-\")\n",
    "\n",
    "    assert(len(temperatureList) == 12)\n",
    "    #df_tempMonthList = pd.DataFrame.from_dict(temperatureList)\n",
    "\n",
    "    header = [\"Year\", \"Month\", \"\", \"Within 10 km\", \"Within 50 km\", \"Within 100 km\", \"Within 250 km\", \"Within 500 km\", \"Within 1000 km\"]\n",
    "    df_counts = pd.read_csv(count_file, delimiter=\"\\s+\", header=None, comment=\"%\", names=header, usecols = [0,1,3,4,5,6,7,8], encoding='latin-1')\n",
    "    header = [\"Year\", \"Month\",  \"Monthly Anomaly\", \"Monthly Unc.\", \"Annual Anomaly\", \"Annual Unc.\",   \"Five-year Anomaly\", \"Five-year Unc.\", \"Ten-year Anomaly\", \"Ten-year Unc.\",  \"Twenty-year Anomaly\", \"Twenty-year Unc.\"]\n",
    "    df_trends = pd.read_csv(trends_file, delimiter=\"\\s+\", header=None, comment=\"%\", names=header, usecols = [0,1,2,3], encoding='latin-1')\n",
    "    \n",
    "    # Drop Records based on the distance of the record stations\n",
    "    #df_counts = df_counts[df_counts[\"Within 500 km\"] > 0]\n",
    "    if len(df_counts) == 0:\n",
    "        return None\n",
    "\n",
    "    df_joined = pd.merge(df_counts, df_trends, how='left', left_on=['Year', 'Month'], right_on=['Year', 'Month'])\n",
    "    df_joined['AverageTemperature'] = df_joined.apply(lambda x: x['Monthly Anomaly'] + temperatureList[x['Month'].astype(int) - 1], axis=1)\n",
    "    df_joined[\"Latitude\"] = latlon[0]\n",
    "    df_joined[\"Longitude\"] = latlon[1]\n",
    "    df_joined[\"Country\"] = country\n",
    "    df_joined[\"City\"] = citys\n",
    "    df_joined[\"dt\"] = df_joined.apply(lambda x: str(x['Year']) + \"-\" + '{0:0>2}'.format(x['Month']) + \"-01\", axis=1)\n",
    "    df_joined.rename(columns={\"Monthly Unc.\": \"AverageTemperatureUncertainty\"}, inplace=True)\n",
    "\n",
    "    #df_joined = df_joined[[\"dt\", \"AverageTemperature\", \"AverageTemperatureUncertainty\", \"City\", \"Country\", \"Latitude\", \"Longitude\"]]\n",
    "    #df_joined = df_joined[[\"dt\", \"AverageTemperature\", \"AverageTemperatureUncertainty\", \"Latitude\", \"Longitude\"]]\n",
    "    return df_joined\n",
    "\n",
    "\n",
    "def getPositionsFromDirectory(dir):\n",
    "    positions = set()\n",
    "    for path in os.listdir(dir):\n",
    "        if not os.path.isfile(os.path.join(dir, path)):\n",
    "            continue\n",
    "        if not \"-TAVG-Trend.txt\" in path:\n",
    "            continue\n",
    "        positions.add(path.replace(\"-TAVG-Trend.txt\", \"\"))\n",
    "    return positions\n",
    "\n",
    "if os.path.isfile(OUTPUT_FILE_BINARY) and not FORCE_RECREATION:\n",
    "    df_merged = pd.read_parquet(OUTPUT_FILE_BINARY)\n",
    "    print(\"Data imported from binary file!!!\")\n",
    "else:\n",
    "    allPositions = getPositionsFromDirectory(LOCAL_FILE_PATH)\n",
    "    print(f\"{len(allPositions)} different Location-Files will be combined\")\n",
    "    df_merged = pd.DataFrame({'A' : []})\n",
    "    pbar = tqdm(total=len(allPositions), desc=\"Merging files\")\n",
    "    for ind, positionName in enumerate(allPositions):\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f\"Merging files ({len(df_merged)} temp entries)\")\n",
    "        #pbar.write(f\"Working on {positionName}...\")\n",
    "        df = getDataForOnePosition(positionName)\n",
    "        if df is None:\n",
    "            continue\n",
    "        if len(df) < 0:\n",
    "            #pbar.write(f\" -> Returned empty dataframe\")\n",
    "            continue\n",
    "        #pbar.write(f\" -> Read {len(df)} entries\")\n",
    "        if len(df_merged) > 0:\n",
    "            df_merged = pd.concat([df_merged, df], ignore_index=True)\n",
    "        else:\n",
    "            df_merged = df\n",
    "    pbar.close()\n",
    "    df_merged.to_parquet(OUTPUT_FILE_BINARY, compression=\"gzip\")\n",
    "\n",
    "df_local = df_merged\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OUTPUT FOR INTERPOLATION\n",
    "The following code outputs the data such that we can further process it with `interpolate_data.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT FOR INTERPOLATION\n",
    "OUTPUT_FILE = \"C:/Users/gkimmersdorfer/Documents/rtvis22-cde/data/sources/GlobalLandTemperaturesByCity.csv\"\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "df_output = df_local\n",
    "df_output['City'] = \"\"\n",
    "df_output['Country'] = \"\"\n",
    "df_output['src'] = 0\n",
    "pd.options.mode.chained_assignment = 'warn'\n",
    "df_output = df_output[[\"dt\", \"AverageTemperature\", \"AverageTemperatureUncertainty\", \"City\", \"Country\", \"Latitude\", \"Longitude\"]]\n",
    "df_output.to_csv(OUTPUT_FILE, index=False, sep=\",\", encoding=\"utf-8\", header=True)\n",
    "print(f\"Output-File created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT WITH DELETED NANS WITHOUT INTERPOLATION READY FOR COMPRESS\n",
    "OUTPUT_FILE = \"C:/Users/gkimmersdorfer/Documents/rtvis22-cde/data/sources/GlobalLandTemperaturesByCity_interpolated.csv\"\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "df_output = df_local.dropna()\n",
    "print(f\"Dropped {len(df_local) - len(df_output)} NaN-entries.\")\n",
    "df_output['City'] = \"\"\n",
    "df_output['Country'] = \"\"\n",
    "df_output['src'] = 0\n",
    "pd.options.mode.chained_assignment = 'warn'\n",
    "\n",
    "df_output = df_output[[\"dt\", \"AverageTemperature\", \"AverageTemperatureUncertainty\", \"City\", \"Country\", \"Latitude\", \"Longitude\", \"src\"]]\n",
    "df_output.to_csv(OUTPUT_FILE, index=False, sep=\",\", encoding=\"utf-8\", header=True)\n",
    "print(f\"Output-File created successfully\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cde_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
